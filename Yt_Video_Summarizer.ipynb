{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Video Summarizer\n",
    "\n",
    "Uses the following tools\n",
    "- LangChain\n",
    "- OpenAi\n",
    "- LLM models\n",
    "- RAG (vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet langchain langchain-community langchain-openai chromadb \n",
    "!pip3 install --quiet python-dotenv\n",
    "!pip3 install --quiet openai \n",
    "!pip3 install youtube-transcript-api\n",
    "!pip3 install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_core.runnables import RunnablePassthrough \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Other modules and packa\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get env var\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "url = input(\"What is the YT video Url?\")\n",
    "question = input(\"Prompt/question about the video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-4o-mini\") #apiKey will be fetched from env variables\n",
    "# print(llm.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #prompt template\n",
    "# EXAMPLE_PROMPT_TEMPLATE = \"\"\"\n",
    "# You are an assistsnt. If you dont know something say that you don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "# Answer the Question: {question}\n",
    "# \"\"\"\n",
    "\n",
    "# example_prompt_template = ChatPromptTemplate.from_template(EXAMPLE_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prompt and get responses EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to format a prompt to pass it on to the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = example_prompt_template.format(question = \"Generate an ai project topic for a beginner. Dont go in detail. Only give 1 title and one line description.\")\n",
    "\n",
    "# print(prompt)\n",
    "# llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='**Title:** \"Simple Chatbot using Python\"  \\n**Description:** Create a basic text-based chatbot that can answer simple questions and engage in conversation with users.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 59, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_709714d124', 'finish_reason': 'stop', 'logprobs': None}, id='run-99b2aaed-9265-43c7-85c8-c5c553812149-0', usage_metadata={'input_tokens': 59, 'output_tokens': 33, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain = (\n",
    "#     {\"question\": RunnablePassthrough()} | example_prompt_template | llm\n",
    "# )\n",
    "\n",
    "# chain.invoke(\"Generate an ai project topic for a beginner. Dont go in detail. Only give 1 title and one line description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get transcript from youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url: str):\n",
    "    loader = YoutubeLoader.from_youtube_url(url)\n",
    "    transcript = loader.load()\n",
    "    return transcript\n",
    "\n",
    "# print(get_transcript(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Transcript and return the list of document chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(docList, chunk_size : int = 1000, chunk_overlap: int = 100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap, length_function = len, separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "    return text_splitter.split_documents(docList)\n",
    "\n",
    "yt_Transcript = split_documents(get_transcript(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Documents and queries\n",
    "We will embed documents and queries to store in the vector DB. \n",
    "\n",
    "The queries will be matched with the documents based on how close the embeddings are to the query in the vector DB (using the euclidian distance). Less the distance, more the relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding_function():\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "# test_vector = embedding_function.embed_query(\"Test string\")\n",
    "# len(test_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check embedding distance between two values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.22374569889973261}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evalauator = load_evaluator(evaluator=\"embedding_distance\", embedding = embedding_function)\n",
    "# evalauator.evaluate_strings(prediction=\"immigration\", reference=\"Canada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(yt_Transcript, embedding_function, directoryName):\n",
    "    vectorStore = Chroma(persist_directory=directoryName, embedding_function=embedding_function)\n",
    "    \n",
    "    # Generate unique IDs based on content hash\n",
    "    def hash_text(text):\n",
    "        return hashlib.md5(text.encode()).hexdigest()  # Hash for consistency\n",
    "\n",
    "    new_texts = []\n",
    "    new_ids = []\n",
    "\n",
    "    for doc in yt_Transcript:\n",
    "        doc_id = hash_text(doc.page_content)\n",
    "        if doc_id not in vectorStore.get()['ids']:  # Check if already exists\n",
    "            new_texts.append(doc.page_content)\n",
    "            new_ids.append(doc_id)\n",
    "\n",
    "    if new_texts:\n",
    "        vectorStore.add_texts(new_texts, ids=new_ids)\n",
    "        vectorStore.persist()\n",
    "\n",
    "    return vectorStore\n",
    "\n",
    "vectorStore = create_db(yt_Transcript, embedding_function, \"vectorDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vector db\n",
    "vectorStore = Chroma(persist_directory=\"vectorDB\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Q&A, the default search type is fine\n",
    "retriever = vectorStore.as_retriever() # kwargs={'k': 6} to get 6 chunks\n",
    "relavantChunks = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use relevant chunks as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistsnt. If you dont know something say that you don't know. DON'T MAKE UP ANYTHING.\n",
    "Use the following context.\n",
    "{context}\n",
    "\n",
    "Answer the Question using the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an answer structre\n",
    "class Response(BaseModel):\n",
    "    Answer : str = Field(description=\"The response from llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1360: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(Answer='The video discusses a project focused on information retrieval and AI, specifically highlighting the process of extracting structured information from unstructured data using tools like Docker and the Lang chain library. \\n\\nKey learnings include:  \\n1. **Information Retrieval Challenges:** The video addresses the tedious and time-consuming nature of extracting insights from reports and documents, which is commonplace in many office jobs.  \\n2. **Role of AI:** As AI evolves, its potential in automating information retrieval and organizing data into structured formats (like Excel) is emphasized.  \\n3. **Lang Chain Framework:** Understanding the Lang chain framework and its core components can open up numerous opportunities in creating intelligent applications.  \\n4. **Practical Guidance:** The presenter plans to guide viewers through various modules and provide coding examples within VS Code, making it accessible even for those unfamiliar with Python.  \\n5. **Resources Available:** There will be a GitHub page linked in the description for viewers to clone the project and follow along, along with instructions for setup and necessary API Keys.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain and use the template with context\n",
    "def format_doc_chunks(chunks):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in chunks)\n",
    "\n",
    "\n",
    "'''\n",
    "retriever returns the chunks to format_doc_chunks which is then passed to context\n",
    "Runnable passthrough returns the question provided when invoking the rag_chain\n",
    "the dictionary is passed to the prompt_template\n",
    "the prompt is then passed to llm\n",
    "'''\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_doc_chunks, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm.with_structured_output(Response, strict=True)\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
